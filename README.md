<div align="center">
 ğŸ‘‹ çš†ã•ã‚“ã€ã“ã‚“ã«ã¡ã¯ï¼ 
    verl ã¯ <b>ByteDance Seed ãƒãƒ¼ãƒ </b>ã«ã‚ˆã£ã¦é–‹å§‹ã•ã‚Œã€verl ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«ã‚ˆã£ã¦ç¶­æŒã•ã‚Œã¦ã„ã‚‹ RL ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚
    <br>
    <br>
</div>

<div align="center">

<a href="https://deepwiki.com/volcengine/verl"><img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" style="height:20px;"></a>
[![GitHub Repo stars](https://img.shields.io/github/stars/volcengine/verl)](https://github.com/volcengine/verl/stargazers)
[![Twitter](https://img.shields.io/twitter/follow/verl_project)](https://twitter.com/verl_project)
<a href="https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA"><img src="https://img.shields.io/badge/Slack-verl-blueviolet?logo=slack&amp"></a>
<a href="https://arxiv.org/pdf/2409.19256"><img src="https://img.shields.io/static/v1?label=EuroSys&message=Paper&color=red"></a>
[![Documentation](https://img.shields.io/badge/documentation-blue)](https://verl.readthedocs.io/en/latest/)
<a href="https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG"><img src="https://img.shields.io/badge/å¾®ä¿¡-green?logo=wechat&amp"></a>

</div>

![seed logo](https://github.com/user-attachments/assets/c42e675e-497c-4508-8bb9-093ad4d1f216)

<h1 style="text-align: center;">verl: Volcano Engine Reinforcement Learning for LLMs</h1>

verl ã¯å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®ãŸã‚ã®æŸ”è»Ÿã§åŠ¹ç‡çš„ã€æœ¬æ ¼é‹ç”¨å¯¾å¿œã® RL ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚

verl ã¯ **[HybridFlow: A Flexible and Efficient RLHF Framework](https://arxiv.org/abs/2409.19256v2)** è«–æ–‡ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ç‰ˆã§ã™ã€‚

verl ã¯æŸ”è»Ÿã§ä½¿ã„ã‚„ã™ãã€ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ï¼š

- **å¤šæ§˜ãª RL ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç°¡å˜ãªæ‹¡å¼µ**: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚Šã€è¤‡é›‘ãªãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã®æŸ”è»Ÿãªè¡¨ç¾ã¨åŠ¹ç‡çš„ãªå®Ÿè¡ŒãŒå¯èƒ½ã€‚GRPOã€PPO ãªã©ã® RL ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã‚’æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã§æ§‹ç¯‰ã§ãã¾ã™ã€‚

- **æ—¢å­˜ LLM ã‚¤ãƒ³ãƒ•ãƒ©ã¨ã®ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ API ã«ã‚ˆã‚‹ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªçµ±åˆ**: è¨ˆç®—ã¨ãƒ‡ãƒ¼ã‚¿ã®ä¾å­˜é–¢ä¿‚ã‚’åˆ†é›¢ã—ã€FSDPã€Megatron-LMã€vLLMã€SGLang ãªã©ã®æ—¢å­˜ LLM ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨ã®ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªçµ±åˆã‚’å®Ÿç¾

- **æŸ”è»Ÿãªãƒ‡ãƒã‚¤ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°**: åŠ¹ç‡çš„ãªãƒªã‚½ãƒ¼ã‚¹åˆ©ç”¨ã¨ç•°ãªã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µã‚¤ã‚ºã§ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã‚’ç•°ãªã‚‹ GPU ã‚»ãƒƒãƒˆã«é…ç½®ã™ã‚‹ã“ã¨ã‚’ã‚µãƒãƒ¼ãƒˆ

- äººæ°—ã® HuggingFace ãƒ¢ãƒ‡ãƒ«ã¨ã®å³åº§ã®çµ±åˆ

verl ã¯é«˜é€Ÿã§ã€ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ï¼š

- **æœ€å…ˆç«¯ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: SOTA LLM ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã®çµ±åˆã€ãŠã‚ˆã³ SOTA RL ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ

- **3D-HybridEngine ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªã‚¢ã‚¯ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«å†åˆ†æ•£**: ãƒ¡ãƒ¢ãƒªå†—é•·æ€§ã‚’æ’é™¤ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚ºé–“ã®é·ç§»æ™‚ã®é€šä¿¡ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å¤§å¹…ã«å‰Šæ¸›

</p>

## ãƒ‹ãƒ¥ãƒ¼ã‚¹
- [2025/07] [ReTool](https://arxiv.org/pdf/2504.11536) ãƒ¬ã‚·ãƒ”ãŒå®Œå…¨ã«ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã•ã‚Œã¾ã—ãŸã€‚[ãƒ–ãƒ­ã‚°](https://www.notion.so/verl-reTool-recipe-Using-multi-round-conversations-and-code-sandboxing-to-improve-the-math-of-large-23a8b5b7feba80b386b2e5b5e3c1cde0)
- [2025/07] åˆå› verl ãƒŸãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãŒ 7æœˆ16æ—¥ã« ICML Vancouver ã§é–‹å‚¬ã•ã‚Œã¾ã™ï¼ICML ã«ã”å‚åŠ ã®æ–¹ã¯[ãœã²ãŠè¶Šã—ãã ã•ã„](https://lu.ma/0ek2nyao)ï¼ï¼ˆç¾åœ°å‚åŠ ã®ã¿ï¼‰
- [2025/07] 7/8 ã® [AWS AI Hours Singapore](https://pages.awscloud.com/aws-ai-hours-sg.html#agenda) ã§ã® verl åŸºèª¿è¬›æ¼”ã€7/11 ã® LF AI & Data Singapore ã«ã‚ˆã‚‹ [Agent for SWE meetup](https://lu.ma/e498qhsi) ã§ã® verl & verl-agent ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ
- [2025/06] Megatron ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ãŸ verl ã«ã‚ˆã‚Šã€[DeepSeek-671b ã‚„ Qwen3-236b](https://verl.readthedocs.io/en/latest/perf/dpsk.html) ãªã©ã®å¤§è¦æ¨¡ MoE ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸ
- [2025/06] verl ãƒãƒ¼ãƒ ãŒ 6æœˆ7æ—¥ã® [PyTorch Day China](https://www.lfasiallc.com/pytorch-day-china/) ã§æœ€æ–°ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚åŒ—äº¬ã§é–‹ç™ºãƒãƒ¼ãƒ ã«ãŠä¼šã„ã—ã¾ã—ã‚‡ã†ï¼
- [2025/04] [Seed-Thinking-v1.5](https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf) æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¾ã—ãŸï¼verl ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸ Seed-Thinking-v1.5 ã¯ã€AIME 2024 ã§ 86.7ã€Codeforces ã§ 55.0ã€GPQA ã§ 77.3 ã‚’é”æˆã—ã€STEM ã¨ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«ãŠã‘ã‚‹å„ªã‚ŒãŸæ¨è«–èƒ½åŠ›ã‚’å®Ÿè¨¼ã—ã¦ã„ã¾ã™ã€‚æ¨è«–ã‚¿ã‚¹ã‚¯ã‚’è¶…ãˆã¦ã€ã“ã®æ‰‹æ³•ã¯å¤šæ§˜ãªé ˜åŸŸã§ã®é¡•è‘—ãªæ±åŒ–ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
- [2025/03] [DAPO](https://dapo-sia.github.io/) ã¯ã€Qwen2.5-32B äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ã„ã¦ AIME 2024 ã§ 50 ãƒã‚¤ãƒ³ãƒˆã‚’é”æˆã—ã€DeepSeek ã® GRPOï¼ˆDeepSeek-R1-Zero-Qwen-32Bï¼‰ã«ã‚ˆã‚‹ä»¥å‰ã® SOTA ã‚’ä¸Šå›ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ SOTA RL ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚DAPO ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯å®Œå…¨ã« verl ã«ã‚ˆã£ã¦æ”¯ãˆã‚‰ã‚Œã¦ãŠã‚Šã€å†ç¾ã‚³ãƒ¼ãƒ‰ã¯ç¾åœ¨ `recipe/dapo` ã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚
<details><summary> more... </summary>
<ul>
  <li> [2025/04] [VAPO](https://arxiv.org/pdf/2504.05118) (value-based augmented PPO) paper covers our latest RL method for reasoning models. Trained from Qwen-32B-base model, VAPO achieves 60.4 on AIME 2024, outperforming DAPO-32B.</li>
  <li>[2025/05] [PF-PPO](https://arxiv.org/abs/2409.06957), accepted to ICML 2025, is now supported in verl! PF-PPO enhances policy learning efficiency and robustness by filtering potentially noisy reward signals and reusing high-quality experiences via a replay buffer.</li>
  <li>[2025/04] We will give a tutorial about latest post-training techniques and programming guide for verl at [ICLR 2025 Expo](https://iclr.cc/virtual/2025/calendar?filter_events=Expo+Talk+Panel&filter_rooms=), [SCI-FM workshop](https://open-foundation-model.github.io/) and [LMSys afterparty](https://lu.ma/d23nyynm). Talk materials available [here](https://github.com/eric-haibin-lin/verl-community/tree/main/iclr25). </li>
  <li>[2025/03] verl v0.3.0.post1 is released! See [release note](https://github.com/volcengine/verl/releases/) for details. It achieves [~1.4x speedup](https://tongyx361.github.io/blogs/posts/verl-intro/#/verl-flexible-and-efficient-rl-for-llms) compared to prev versions.</li>
  <li>[2025/05] verl will be presented at [A2M Shanghai](https://a2m.msup.com.cn/home/?aid=4488&city=shanghai) on 5/16 - 5/17.</li>
  <li>[2025/05] verl will be presented at [GOSIM x PyTorch Day 2025](https://paris2025.gosim.org/). See you in Paris! </li>
  <li>[2025/03] We introduced the programming model of verl at the [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg) and [verl intro and updates](https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl-lmsys-meetup.pdf) at the [SGLang-LMSYS Org Meetup](https://lu.ma/ntjrr7ig) in Sunnyvale mid-March.</li>
  <li>[2025/03] We will present verl(HybridFlow) at EuroSys 2025. See you in Rotterdam!</li>
  <li>[2025/02] verl v0.2.0.post2 is released!</li>
  <li>[2025/02] We presented verl in the <a href="https://lu.ma/ji7atxux">Bytedance/NVIDIA/Anyscale Ray Meetup</a>. See you in San Jose!</li>
  <li>[2025/01] [Doubao-1.5-pro](https://team.doubao.com/zh/special/doubao_1_5_pro) is released with SOTA-level performance on LLM & VLM. The RL scaling preview model is trained using verl, reaching OpenAI O1-level performance on math benchmarks (70.0 pass@1 on AIME).</li>
  <li>[2024/12] verl is presented at Ray Forward 2024. Slides available <a href="https://github.com/eric-haibin-lin/verl-community/blob/main/slides/Ray_Forward_2024_%E5%B7%AB%E9%94%A1%E6%96%8C.pdf">here</a></li>
  <li>[2024/12] The team presented <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">Post-training LLMs: From Algorithms to Infrastructure</a> at NeurIPS 2024. <a href="https://github.com/eric-haibin-lin/verl-data/tree/neurips">Slides</a> and <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">video</a> available.</li>
  <li>[2024/10] verl is presented at Ray Summit. <a href="https://www.youtube.com/watch?v=MrhMcXkXvJU&list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&index=37">Youtube video</a> available.</li>
  <li>[2024/08] HybridFlow (verl) is accepted to EuroSys 2025.</li>
</ul>   
</details>

## ä¸»ãªæ©Ÿèƒ½

- ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã® **FSDP**ã€**FSDP2**ã€**Megatron-LM**
- ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆç”Ÿæˆç”¨ã® **vLLM**ã€**SGLang**ã€**HF Transformers**
- Hugging Face Transformers ã¨ Modelscope Hub ã¨ã®äº’æ›æ€§ï¼š[Qwen-3](https://github.com/volcengine/verl/blob/main/examples/grpo_trainer/run_qwen3-8b.sh)ã€Qwen-2.5ã€Llama3.1ã€Gemma2ã€DeepSeek-LLM ãªã©
- æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- [PPO](examples/ppo_trainer/)ã€[GRPO](examples/grpo_trainer/)ã€[ReMax](examples/remax_trainer/)ã€[REINFORCE++](https://verl.readthedocs.io/en/latest/examples/config.html#algorithm)ã€[RLOO](examples/rloo_trainer/)ã€[PRIME](recipe/prime/)ã€[DAPO](recipe/dapo/)ã€[DrGRPO](recipe/drgrpo)ã€[KL_Cov & Clip_Cov](recipe/entropy) ãªã©ã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’
  - æ•°å­¦ã€[ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°](https://github.com/volcengine/verl/tree/main/recipe/dapo) ãªã©ã®ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹å ±é…¬ã¨é–¢æ•°ãƒ™ãƒ¼ã‚¹å ±é…¬ï¼ˆæ¤œè¨¼å¯èƒ½ãªå ±é…¬ï¼‰ã‚’ã‚µãƒãƒ¼ãƒˆ
  - Qwen2.5-vlã€Kimi-VL ã«ã‚ˆã‚‹è¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ã¨[ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« RL](examples/grpo_trainer/run_qwen2_5_vl-7b.sh) ã‚’ã‚µãƒãƒ¼ãƒˆ
  - [ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’ä¼´ã†ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³](https://github.com/volcengine/verl/tree/main/examples/sglang_multiturn)
- [è‡ªå·±å¯¾æˆ¦é¸å¥½æœ€é©åŒ–ï¼ˆSPPOï¼‰](https://github.com/volcengine/verl/tree/main/recipe/sppo) ãªã©ã® LLM ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ¬ã‚·ãƒ”
- Flash attention 2ã€[ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‘ãƒƒã‚­ãƒ³ã‚°](examples/ppo_trainer/run_qwen2-7b_seq_balance.sh)ã€DeepSpeed Ulysses ã«ã‚ˆã‚‹[ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—åŒ–](examples/ppo_trainer/run_deepseek7b_llm_sp2.sh)ã€[LoRA](examples/sft/gsm8k/run_qwen_05_peft.sh)ã€[Liger-kernel](examples/sft/gsm8k/run_qwen_05_sp2_liger.sh) ã®ã‚µãƒãƒ¼ãƒˆ
- [ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆä¸¦åˆ—åŒ–](https://github.com/volcengine/verl/pull/1467)ã«ã‚ˆã‚Š 671B ãƒ¢ãƒ‡ãƒ«ã¨æ•°ç™¾ã® GPU ã¾ã§ã‚¹ã‚±ãƒ¼ãƒ«
- ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚ã®ãƒãƒ«ãƒ GPU [LoRA RL](https://verl.readthedocs.io/en/latest/advance/ppo_lora.html) ã‚µãƒãƒ¼ãƒˆ
- wandbã€swanlabã€mlflowã€tensorboard ã«ã‚ˆã‚‹å®Ÿé¨“è¿½è·¡

## Upcoming Features and Changes

- Q3 Roadmap https://github.com/volcengine/verl/issues/2388
- DeepSeek 671b optimizations with Megatron https://github.com/volcengine/verl/issues/1033
- Multi-turn rollout and tools using optimizations https://github.com/volcengine/verl/issues/1882
- [Agent integration](https://github.com/volcengine/verl/tree/main/verl/experimental/agent_loop)
- Async and off-policy architecture https://github.com/volcengine/verl/pull/2231
- List of breaking changes since v0.4 https://github.com/volcengine/verl/discussions/2270

## ã¯ã˜ã‚ã«

<a href="https://verl.readthedocs.io/en/latest/index.html"><b>ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ</b></a> | <a href="docs_ja/README.md"><b>æ—¥æœ¬èªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ</b></a>

**ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ:**

- [ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](https://verl.readthedocs.io/en/latest/start/install.html) | [æ—¥æœ¬èªç‰ˆ](docs_ja/start/install.md)
- [ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ](https://verl.readthedocs.io/en/latest/start/quickstart.html) | [æ—¥æœ¬èªç‰ˆ](docs_ja/start/quickstart.md)
- [ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚¬ã‚¤ãƒ‰](https://verl.readthedocs.io/en/latest/hybrid_flow.html) & [æŠ€è¡“è¬›æ¼”](https://hcqnc.xetlk.com/sl/3vACOK)ï¼ˆä¸­å›½èªï¼‰| [æ—¥æœ¬èªç‰ˆ](docs_ja/hybrid_flow.md)
- [verl ã§ã® PPO](https://verl.readthedocs.io/en/latest/algo/ppo.html) | [æ—¥æœ¬èªç‰ˆ](docs_ja/algo/ppo.md)
- [verl ã§ã® GRPO](https://verl.readthedocs.io/en/latest/algo/grpo.html) | [æ—¥æœ¬èªç‰ˆ](docs_ja/algo/grpo.md)

**PPO ã‚µãƒ³ãƒ—ãƒ«ã®ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ:**

- [ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™](https://verl.readthedocs.io/en/latest/preparation/prepare_data.html) | [æ—¥æœ¬èªç‰ˆ](docs_ja/preparation/prepare_data.md)
- [ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨å ±é…¬é–¢æ•°ã®å®Ÿè£…](https://verl.readthedocs.io/en/latest/preparation/reward_function.html) | [æ—¥æœ¬èªç‰ˆ](docs_ja/preparation/reward_function.md)
- [PPO ã‚µãƒ³ãƒ—ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html) | [æ—¥æœ¬èªç‰ˆ](docs_ja/examples/ppo_code_architecture.md)
- [è¨­å®šã®èª¬æ˜](https://verl.readthedocs.io/en/latest/examples/config.html) | [æ—¥æœ¬èªç‰ˆ](docs_ja/examples/config.md)

**å†ç¾å¯èƒ½ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:**

- [ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€æ•°å­¦ã§ã® RL ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹](https://verl.readthedocs.io/en/latest/algo/baseline.html) | [æ—¥æœ¬èªç‰ˆ](docs_ja/algo/baseline.md)

**For code explanation and advance usage (extension):**

- PPO Trainer and Workers
  - [PPO Ray Trainer](https://verl.readthedocs.io/en/latest/workers/ray_trainer.html)
  - [PyTorch FSDP Backend](https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html)
  - [Megatron-LM Backend](https://verl.readthedocs.io/en/latest/index.html)

- Advanced Usage and Extension
  - [Add Models with the FSDP Backend](https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html)
  - [Add Models with the Megatron-LM Backend](https://verl.readthedocs.io/en/latest/advance/megatron_extension.html)
  - [Multi-turn Rollout Support](https://verl.readthedocs.io/en/latest/sglang_multiturn/multiturn.html)
  - [Search Tool Integration](https://verl.readthedocs.io/en/latest/sglang_multiturn/search_tool_example.html)
  - [Sandbox Fusion Integration](https://verl.readthedocs.io/en/latest/examples/sandbox_fusion_example.html)
  - [Deployment using Separate GPU Resources](https://github.com/volcengine/verl/tree/main/examples/split_placement)
  - [Extend to Other RL(HF) algorithms](https://verl.readthedocs.io/en/latest/advance/dpo_extension.html)
  - [Ray API design tutorial](https://verl.readthedocs.io/en/latest/advance/placement.html)

**Blogs from the community**

- [When Reasoning Models Break Tokenization: The Hidden Complexity of Multiturn Training](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/fast_tokenization/multiturn_tokenization_and_masking.md)
- [verl deployment on AWS SageMaker](https://medium.com/@kaige.yang0110/run-verl-on-sagemaker-using-4x8-l40s-gpus-8e6d5c3c61d3)
- [verl x SGLang Multi-turn Code Walkthrough](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/code-walk-through/readme_EN.md)
- [Optimizing SGLang Memory Usage in verl](https://hebiao064.github.io/rl-memory-management)
- [SGLang, verl, OpenBMB and Tsinghua University: Pioneering End-to-End Multi-Turn RLHF](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/verl-multiturn-rollout-Release.md)
- [Reinforcement Learning from Human Feedback on AMD GPUs with verl and ROCm Integration](https://rocm.blogs.amd.com/artificial-intelligence/verl-large-scale/README.html)
- [veMLP x verl ï¼šç©è½¬å¼ºåŒ–å­¦ä¹ è®­ç»ƒ](https://mp.weixin.qq.com/s/7nbqxk4knMGd-hQE9ls2tA)
- [ä½¿ç”¨ verl è¿›è¡Œ GRPO åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœ€ä½³å®è·µ](https://www.volcengine.com/docs/6459/1463942)
- [HybridFlow verl åŸæ–‡æµ…æ](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md)
- [æœ€é«˜æå‡ 20 å€ååé‡ï¼è±†åŒ…å¤§æ¨¡å‹å›¢é˜Ÿå‘å¸ƒå…¨æ–° RLHF æ¡†æ¶ï¼Œç°å·²å¼€æºï¼](https://team.doubao.com/en/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90)

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼ RL ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¨ã£ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ä¸å¯æ¬ ã§ã™ã€‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã€è©³ç´°ãª[ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰](https://verl.readthedocs.io/en/latest/perf/perf_tuning.html)ï¼ˆ[æ—¥æœ¬èªç‰ˆ](docs_ja/web_content/performance_tuning_guide.md)ï¼‰ã‚’ä½œæˆã—ã¾ã—ãŸã€‚

## Upgrade to vLLM >= v0.8.2

verl now supports vLLM>=0.8.2 when using FSDP as the training backend. Please refer to [this document](https://github.com/volcengine/verl/blob/main/docs/README_vllm0.8.md) for the installation guide and more information. Please avoid vllm 0.7.x, which contains bugs that may lead to OOMs and unexpected errors.

## Use Latest SGLang

SGLang is fully supported with verl, and SGLang RL Group is working extensively on building unique features, including multi-turn agentic RL, VLM RLHF, server-based RL, and partial rollout. Please refer to [this document](https://verl.readthedocs.io/en/latest/workers/sglang_worker.html) for the installation guide and more information.

## Upgrade to FSDP2

verl is fully embracing FSDP2! FSDP2 is recommended by torch distributed team, providing better throughput and memory usage, and is composible with other features (e.g. torch.compile). To enable FSDP2, simply use verl main and set the following options:
```
actor_rollout_ref.ref.strategy=fsdp2
actor_rollout_ref.actor.strategy=fsdp2
critic.strategy=fsdp2 
reward_model.strategy=fsdp2 
```
Furthermore, FSDP2 cpu offloading is compatible with gradient accumulation. You can turn it on to save memory with `actor_rollout_ref.actor.fsdp_config.offload_policy=True`. For more details, see https://github.com/volcengine/verl/pull/1026

## AMD Support (ROCm Kernel)

verl now supports FSDP as the training engine (Megatron support coming soon) and both integrates with vLLM and SGLang as inference engines. Please refer to [this document](https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_build_dockerfile_page.rst) for the installation guide and more information, and [this document](https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_vllm_page.rst) for the vLLM performance tuning for ROCm.


## å¼•ç”¨ã¨è¬è¾

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒå½¹ç«‹ã¤ã¨æ€ã‚ã‚Œã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ï¼š

- [HybridFlow: A Flexible and Efficient RLHF Framework](https://arxiv.org/abs/2409.19256v2)
- [A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization](https://i.cs.hku.hk/~cwu/papers/gmsheng-NL2Code24.pdf)

```bibtex
@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}
```

verl is inspired by the design of Nemo-Aligner, Deepspeed-chat and OpenRLHF. The project is adopted and contributed by Bytedance, Anyscale, LMSys.org, [Alibaba Qwen team](https://github.com/QwenLM/), Shanghai AI Lab, Tsinghua University, UC Berkeley, UCLA, UIUC, University of Hong Kong, ke.com, [All Hands AI](https://www.all-hands.dev/), [ModelBest](http://modelbest.cn/), JD AI Lab, Microsoft Research, [StepFun](https://www.stepfun.com/), Amazon, LinkedIn, Meituan, [Camel-AI](https://www.camel-ai.org/), [OpenManus](https://github.com/OpenManus), Xiaomi, NVIDIA research, [Baichuan](https://www.baichuan-ai.com/home), [RedNote](https://www.xiaohongshu.com/), [SwissAI](https://www.swiss-ai.org/), [Moonshot AI (Kimi)](https://www.moonshot-ai.com/), Baidu, Snowflake, Skywork.ai, JetBrains, [IceSword Lab](https://www.iceswordlab.com), and many more.

## Awesome work using verl

- [TinyZero](https://github.com/Jiayi-Pan/TinyZero): a reproduction of **DeepSeek R1 Zero** recipe for reasoning tasks ![GitHub Repo stars](https://img.shields.io/github/stars/Jiayi-Pan/TinyZero)
- [SkyThought](https://github.com/NovaSky-AI/SkyThought): RL training for Sky-T1-7B by NovaSky AI team. ![GitHub Repo stars](https://img.shields.io/github/stars/NovaSky-AI/SkyThought)
- [simpleRL-reason](https://github.com/hkust-nlp/simpleRL-reason): SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild ![GitHub Repo stars](https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason)
- [Easy-R1](https://github.com/hiyouga/EasyR1): **Multi-modal** RL training framework ![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/EasyR1)
- [OpenManus-RL](https://github.com/OpenManus/OpenManus-RL): LLM Agents RL tunning framework for multiple agent environments. ![GitHub Repo stars](https://img.shields.io/github/stars/OpenManus/OpenManus-RL)
- [rllm](https://github.com/agentica-project/rllm): async RL training with [verl-pipeline](https://github.com/agentica-project/verl-pipeline) ![GitHub Repo stars](https://img.shields.io/github/stars/agentica-project/rllm)
- [RAGEN](https://github.com/ZihanWang314/ragen): a general-purpose reasoning **agent** training framework ![GitHub Repo stars](https://img.shields.io/github/stars/ZihanWang314/ragen)
- [Search-R1](https://github.com/PeterGriffinJin/Search-R1): RL with reasoning and **searching (tool-call)** interleaved LLMs ![GitHub Repo stars](https://img.shields.io/github/stars/PeterGriffinJin/Search-R1)
- [ReSearch](https://github.com/Agent-RL/ReSearch): Learning to **Re**ason with **Search** for LLMs via Reinforcement Learning ![GitHub Repo stars](https://img.shields.io/github/stars/Agent-RL/ReSearch)
- [Skywork-OR1](https://github.com/SkyworkAI/Skywork-OR1): Skywork open reaonser series ![GitHub Repo stars](https://img.shields.io/github/stars/SkyworkAI/Skywork-OR1)
- [ToRL](https://github.com/GAIR-NLP/ToRL): Scaling tool-integrated RL ![GitHub Repo stars](https://img.shields.io/github/stars/GAIR-NLP/ToRL)
- [Absolute Zero Reasoner](https://github.com/LeapLabTHU/Absolute-Zero-Reasoner): [A no human curated data self-play framework for reasoning](https://arxiv.org/abs/2505.03335) ![GitHub Repo stars](https://img.shields.io/github/stars/LeapLabTHU/Absolute-Zero-Reasoner)
- [verl-agent](https://github.com/langfengQ/verl-agent): A scalable training framework for **long-horizon LLM/VLM agents**, along with a new algorithm **GiGPO** ![GitHub Repo stars](https://img.shields.io/github/stars/langfengQ/verl-agent)
- [RL-Factory](https://github.com/Simple-Efficient/RL-Factory): An easy and efficient RL post-training framework for Agentic Learning ![GitHub Repo stars](https://img.shields.io/github/stars/Simple-Efficient/RL-Factory)
- [ReTool](https://retool-rl.github.io/): ReTool: reinforcement learning for strategic tool use in LLMs. Code release is in progress...
- [verl-tool](https://github.com/TIGER-AI-Lab/verl-tool): An unified and easy-to-extend tool-agent training framework based on verl![GitHub Repo stars](https://img.shields.io/github/stars/TIGER-AI-Lab/verl-tool)
- [PRIME](https://github.com/PRIME-RL/PRIME): Process reinforcement through implicit rewards ![GitHub Repo stars](https://img.shields.io/github/stars/PRIME-RL/PRIME)
- [MemAgent](https://github.com/BytedTsinghua-SIA/MemAgent): MemAgent: Reshaping Long-Context LLM with Multi-Conv RL based Memory Agent ![GitHub Repo stars](https://img.shields.io/github/stars/BytedTsinghua-SIA/MemAgent)
- [POLARIS](https://github.com/ChenxinAn-fdu/POLARIS): A Post-training recipe for scaling RL on Advanced Reasoning models ![GitHub Repo stars](https://img.shields.io/github/stars/ChenxinAn-fdu/POLARIS)
- [GUI-R1](https://github.com/ritzz-ai/GUI-R1): **GUI-R1**: A Generalist R1-style Vision-Language Action Model For **GUI Agents** ![GitHub Repo stars](https://img.shields.io/github/stars/ritzz-ai/GUI-R1)
- [DeepRetrieval](https://github.com/pat-jj/DeepRetrieval): RL Training of **Search Agent** with **Search/Retrieval Outcome** ![GitHub Repo stars](https://img.shields.io/github/stars/pat-jj/DeepRetrieval)
- [Code-R1](https://github.com/ganler/code-r1): Reproducing R1 for **Code** with Reliable Rewards ![GitHub Repo stars](https://img.shields.io/github/stars/ganler/code-r1)
- [DeepResearcher](https://github.com/GAIR-NLP/DeepResearcher): Scaling deep research via reinforcement learning in real-world environments ![GitHub Repo stars](https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher)
- [VAGEN](https://github.com/RAGEN-AI/VAGEN): Training VLM agents with multi-turn reinforcement learning ![GitHub Repo stars](https://img.shields.io/github/stars/RAGEN-AI/VAGEN)
- [RM-R1](https://arxiv.org/abs/2505.02387): RL training of reasoning reward models ![GitHub Repo stars](https://img.shields.io/github/stars/RM-R1-UIUC/RM-R1)
- [LUFFY](https://arxiv.org/pdf/2504.14945): Learning to Reason under Off-Policy Guidance![GitHub Repo stars](https://img.shields.io/github/stars/ElliottYan/LUFFY)
- [DeepMath](https://github.com/zwhe99/DeepMath): DeepMath-103K data and series models for math reasoning![GitHub Repo stars](https://img.shields.io/github/stars/zwhe99/DeepMath)
- [Entropy Mechanism of RL](https://github.com/PRIME-RL/Entropy-Mechanism-of-RL): The Entropy Mechanism of Reinforcement Learning for Large Language Model Reasoning![GitHub Repo stars](https://img.shields.io/github/stars/PRIME-RL/Entropy-Mechanism-of-RL)
- [LLaSA-TTS-GRPO](https://github.com/channel-io/ch-tts-llasa-rl-grpo): TTS fine-tuning with GRPO optimization based on LLASA models ![GitHub Repo stars](https://img.shields.io/github/stars/channel-io/ch-tts-llasa-rl-grpo)
- [PF-PPO](https://arxiv.org/abs/2409.06957): Policy Filtration for PPO based on the reliability of reward signals for more efficient and robust RLHF.
- [RACRO](https://github.com/gyhdog99/RACRO2): Build multi-modal reasoning models via decoupling it into query-conditioned captioning and text-only reasoning ![GitHub Repo stars](https://img.shields.io/github/stars/gyhdog99/RACRO2)
- [Agent Lightning](https://github.com/microsoft/agent-lightning): A flexible and extensible framework that enables seamless agent optimization for any existing agent framework. ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/agent-lightning)

and many more awesome work listed in [recipe](recipe/README.md).

## è²¢çŒ®ã‚¬ã‚¤ãƒ‰

[è²¢çŒ®ã‚¬ã‚¤ãƒ‰](CONTRIBUTING.md)ï¼ˆ[æ—¥æœ¬èªç‰ˆ](CONTRIBUTING_ja.md)ï¼‰ã‚’ã”è¦§ãã ã•ã„

## About [ByteDance Seed Team](https://team.doubao.com/)

Founded in 2023, ByteDance Seed Team is dedicated to crafting the industry's most advanced AI foundation models. The team aspires to become a world-class research team and make significant contributions to the advancement of science and society. You can get to know Bytedance Seed better through the following channelsğŸ‘‡
<div>
  <a href="https://team.doubao.com/">
    <img src="https://img.shields.io/badge/Website-%231e37ff?style=for-the-badge&logo=bytedance&logoColor=white"></a>
  <a href="https://github.com/user-attachments/assets/469535a8-42f2-4797-acdf-4f7a1d4a0c3e">
    <img src="https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&logo=wechat&logoColor=white"></a>
 <a href="https://www.xiaohongshu.com/user/profile/668e7e15000000000303157d?xsec_token=ABl2-aqekpytY6A8TuxjrwnZskU-6BsMRE_ufQQaSAvjc%3D&xsec_source=pc_search">
    <img src="https://img.shields.io/badge/Xiaohongshu-%23FF2442?style=for-the-badge&logo=xiaohongshu&logoColor=white"></a>
  <a href="https://www.zhihu.com/org/dou-bao-da-mo-xing-tuan-dui/">
    <img src="https://img.shields.io/badge/zhihu-%230084FF?style=for-the-badge&logo=zhihu&logoColor=white"></a>

</div>
---

We are HIRING! Send us an [email](mailto:haibin.lin@bytedance.com) if you are interested in internship/FTE opportunities in RL for agents.
