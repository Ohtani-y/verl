{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0ddc582b",
      "metadata": {
        "id": "0ddc582b"
      },
      "source": [
        "# VeRL Ray API Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71fe3b94",
      "metadata": {
        "id": "71fe3b94"
      },
      "source": [
        "## Chapter 1: Ray Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1347d381",
      "metadata": {
        "tags": [],
        "id": "1347d381"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e75b9d44",
      "metadata": {
        "tags": [],
        "id": "e75b9d44"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "!pip install ray\n",
        "import ray\n",
        "import torch\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e90ae00",
      "metadata": {
        "tags": [],
        "id": "2e90ae00"
      },
      "outputs": [],
      "source": [
        "# Build a local ray cluster. The head node and worker node are on this machine\n",
        "ray.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a127e4e4",
      "metadata": {
        "id": "a127e4e4"
      },
      "source": [
        "Implement an Accumulator class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20e7b9a3",
      "metadata": {
        "tags": [],
        "id": "20e7b9a3"
      },
      "outputs": [],
      "source": [
        "@ray.remote\n",
        "class Accumulator:\n",
        "    def __init__(self):\n",
        "        self.value = 0\n",
        "\n",
        "    def add(self, x):\n",
        "        self.value += x\n",
        "\n",
        "    def get_value(self):\n",
        "        return self.value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b80098c",
      "metadata": {
        "tags": [],
        "id": "3b80098c"
      },
      "outputs": [],
      "source": [
        "# Instantiate an accumulator. Accumulator can be viewed as a process, acting as an RPC service.\n",
        "accumulator = Accumulator.remote()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14b1009",
      "metadata": {
        "tags": [],
        "id": "b14b1009"
      },
      "outputs": [],
      "source": [
        "value_ref = accumulator.get_value.remote()  # Check the current value. Note that this function returns immediately and does not actually wait for the remote execution to complete.\n",
        "# Get the value\n",
        "value = ray.get(value_ref)\n",
        "print(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "513a84b3",
      "metadata": {
        "tags": [],
        "id": "513a84b3"
      },
      "outputs": [],
      "source": [
        "# Accumulate, then check the result.\n",
        "accumulator.add.remote(10)  # Similarly, the 'add' here will return immediately.\n",
        "new_value = ray.get(accumulator.get_value.remote())\n",
        "print(new_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c332fe0",
      "metadata": {
        "id": "3c332fe0"
      },
      "source": [
        "## Chapter 2: Resource Pool and RayWorkerGroup\n",
        "In the previous example, it was a simple single-process worker.\n",
        "In this example, we implement a worker with a GPU and form a RayWorkerGroup. Within this RayWorkerGroup, we implement a simple operation of an accumulator."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZQTuSCegoNf-"
      },
      "id": "ZQTuSCegoNf-",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install verl\n",
        "# Reinstall torch to ensure compatibility with the numpy version installed by verl.\n",
        "!pip install --upgrade --force-reinstall torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_N13gjsoH4m",
        "outputId": "61cbe772-6b6a-474e-e0be-ee0e70bda76c"
      },
      "id": "V_N13gjsoH4m",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: verl in /usr/local/lib/python3.12/dist-packages (0.7.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from verl) (1.12.0)\n",
            "Requirement already satisfied: codetiming in /usr/local/lib/python3.12/dist-packages (from verl) (1.4.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from verl) (4.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from verl) (0.3.8)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.12/dist-packages (from verl) (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.12/dist-packages (from verl) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from verl) (2.2.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (from verl) (0.18.1)\n",
            "Requirement already satisfied: pyarrow>=19.0.0 in /usr/local/lib/python3.12/dist-packages (from verl) (23.0.0)\n",
            "Requirement already satisfied: pybind11 in /usr/local/lib/python3.12/dist-packages (from verl) (3.0.1)\n",
            "Requirement already satisfied: pylatexenc in /usr/local/lib/python3.12/dist-packages (from verl) (2.10)\n",
            "Requirement already satisfied: ray>=2.41.0 in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.41.0->verl) (2.53.0)\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.12/dist-packages (from verl) (0.11.0)\n",
            "Requirement already satisfied: tensordict!=0.9.0,<=0.10.0,>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from verl) (0.10.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from verl) (5.0.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from verl) (0.24.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from verl) (25.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from verl) (2.19.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.41.0->ray[default]>=2.41.0->verl) (8.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray>=2.41.0->ray[default]>=2.41.0->verl) (3.24.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray>=2.41.0->ray[default]>=2.41.0->verl) (4.26.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.41.0->ray[default]>=2.41.0->verl) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from ray>=2.41.0->ray[default]>=2.41.0->verl) (5.29.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from ray>=2.41.0->ray[default]>=2.41.0->verl) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ray>=2.41.0->ray[default]>=2.41.0->verl) (2.32.4)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.41.0->verl) (3.13.3)\n",
            "Requirement already satisfied: aiohttp_cors in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.41.0->verl) (0.8.1)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.41.0->verl) (0.5.8)\n",
            "Requirement already satisfied: py-spy>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.41.0->verl) (0.4.1)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.41.0->verl) (1.76.0)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.41.0->verl) (0.11.4)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.30.0 in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.41.0->verl) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-prometheus in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.41.0->verl) (0.59b0)\n",
            "Requirement already satisfied: opentelemetry-proto in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.41.0->verl) (1.38.0)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3 in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.41.0->verl) (2.12.3)\n",
            "Requirement already satisfied: prometheus_client>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.41.0->verl) (0.24.1)\n",
            "Requirement already satisfied: smart_open in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.41.0->verl) (7.5.0)\n",
            "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /usr/local/lib/python3.12/dist-packages (from ray[default]>=2.41.0->verl) (20.36.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (2.10.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (3.1.2)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (8.7.1)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.12/dist-packages (from tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (3.11.7)\n",
            "Requirement already satisfied: pyvers<0.2.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (0.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->verl) (5.9.5)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->verl) (1.4.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate->verl) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets->verl) (4.67.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->verl) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->verl) (0.70.16)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->verl)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core->verl) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core->verl) (4.9.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->verl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->verl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->verl) (2025.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->verl) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->verl) (3.10.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->verl) (82.0.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->verl) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->verl) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->verl) (3.1.5)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.12/dist-packages (from torchdata->verl) (2.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->verl) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->verl) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers->verl) (0.21.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->verl) (3.1.46)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->verl) (4.5.1)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->verl) (2.52.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb->verl) (4.15.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7->ray[default]>=2.41.0->verl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7->ray[default]>=2.41.0->verl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7->ray[default]>=2.41.0->verl) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7->ray[default]>=2.41.0->verl) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7->ray[default]>=2.41.0->verl) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7->ray[default]>=2.41.0->verl) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.7->ray[default]>=2.41.0->verl) (1.22.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->verl) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate->verl) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate->verl) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate->verl) (1.5.4)\n",
            "Requirement already satisfied: opentelemetry-api==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.30.0->ray[default]>=2.41.0->verl) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.30.0->ray[default]>=2.41.0->verl) (0.59b0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (3.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3->ray[default]>=2.41.0->verl) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3->ray[default]>=2.41.0->verl) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.10.*,!=2.11.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3->ray[default]>=2.41.0->verl) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->ray>=2.41.0->ray[default]>=2.41.0->verl) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->ray>=2.41.0->ray[default]>=2.41.0->verl) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->ray>=2.41.0->ray[default]>=2.41.0->verl) (2026.1.4)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (3.1.6)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (1.3.4)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.12/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default]>=2.41.0->verl) (0.4.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->verl) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray>=2.41.0->ray[default]>=2.41.0->verl) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray>=2.41.0->ray[default]>=2.41.0->verl) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray>=2.41.0->ray[default]>=2.41.0->verl) (0.30.0)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from opencensus->ray[default]>=2.41.0->verl) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from opencensus->ray[default]>=2.41.0->verl) (2.29.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open->ray[default]>=2.41.0->verl) (2.1.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->verl) (5.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.41.0->verl) (1.72.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.41.0->verl) (1.27.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.41.0->verl) (2.47.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate->verl) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate->verl) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate->verl) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->tensordict!=0.9.0,<=0.10.0,>=0.8.0->verl) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.41.0->verl) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.41.0->verl) (4.9.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=2.41.0->verl) (0.6.2)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2026.2.0\n",
            "    Uninstalling fsspec-2026.2.0:\n",
            "      Successfully uninstalled fsspec-2026.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.24.0+cu128 requires torch==2.9.0, but you have torch 2.10.0 which is incompatible.\n",
            "torchaudio 2.9.0+cu128 requires torch==2.9.0, but you have torch 2.10.0 which is incompatible.\n",
            "fastai 2.8.6 requires torch<2.10,>=1.10, but you have torch 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2025.3.0\n",
            "Collecting torch\n",
            "  Using cached torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (31 kB)\n",
            "Collecting filelock (from torch)\n",
            "  Using cached filelock-3.24.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting setuptools (from torch)\n",
            "  Using cached setuptools-82.0.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx>=2.5.1 (from torch)\n",
            "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting jinja2 (from torch)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec>=0.8.5 (from torch)\n",
            "  Using cached fsspec-2026.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting cuda-bindings==12.9.4 (from torch)\n",
            "  Using cached cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
            "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvshmem-cu12==3.4.5 (from torch)\n",
            "  Using cached nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
            "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.6.0 (from torch)\n",
            "  Using cached triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting cuda-pathfinder~=1.1 (from cuda-bindings==12.9.4->torch)\n",
            "  Using cached cuda_pathfinder-1.3.4-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
            "  Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Using cached torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl (915.7 MB)\n",
            "Using cached cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.2 MB)\n",
            "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[33m(autoscaler +6m23s)\u001b[0m Error: No available node types can fulfill resource request defaultdict(<class 'float'>, {'CPU': 40.0, 'GPU': 4.0}). Add suitable node types to this cluster to resolve this issue.\n",
            "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "Using cached nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (139.1 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "04229afb"
      },
      "outputs": [],
      "source": [
        "from verl.single_controller.base import Worker\n",
        "from verl.single_controller.ray.base import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup, merge_resource_pool"
      ],
      "id": "04229afb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d0dbd58",
      "metadata": {
        "tags": [],
        "id": "0d0dbd58"
      },
      "outputs": [],
      "source": [
        "resource_pool = RayResourcePool([4], use_gpu=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68f6838a",
      "metadata": {
        "tags": [],
        "id": "68f6838a"
      },
      "outputs": [],
      "source": [
        "@ray.remote\n",
        "class GPUAccumulator(Worker):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        # The initial value of each rank is the same as the rank\n",
        "        self.value = torch.zeros(size=(1,), device=\"cuda\") + self.rank\n",
        "\n",
        "    def add(self, x):\n",
        "        self.value += x\n",
        "        print(f\"rank {self.rank}, value: {self.value}\")\n",
        "        return self.value.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23aad8fe",
      "metadata": {
        "tags": [],
        "id": "23aad8fe"
      },
      "outputs": [],
      "source": [
        "# Each worker's initial value is its rank, and then each rank's value is incremented by 1, so the values obtained on each rank are [1, 2, 3, 4]\n",
        "class_with_args = RayClassWithInitArgs(cls=GPUAccumulator)\n",
        "worker_group = RayWorkerGroup(resource_pool, class_with_args)\n",
        "print(worker_group.execute_all_sync(\"add\", x=[1, 1, 1, 1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6705284",
      "metadata": {
        "id": "e6705284"
      },
      "source": [
        "The principle of parameter passing: The input parameter is a list of length world_size, where each element in the list is dispatched respectively to each worker in the RayWorkerGroup.\n",
        "The return parameter is also a list, corresponding to the return value of each worker."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d25c2412",
      "metadata": {
        "id": "d25c2412"
      },
      "source": [
        "### GPU Resource Sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f74f6d24",
      "metadata": {
        "id": "f74f6d24"
      },
      "source": [
        "RayWorkerGroups mapped to the same resource pool share the GPU. In this example, we implement three resource pools: the first occupies 4 GPUs, the second also occupies 4 GPUs, and the last occupies all 8 GPUs. Among them, the first resource pool reuses the resource pool mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49f9c06f",
      "metadata": {
        "tags": [],
        "id": "49f9c06f"
      },
      "outputs": [],
      "source": [
        "# Create a new resource pool and then merge the newly created resource pool with the previous one.\n",
        "resource_pool_1 = RayResourcePool([4], use_gpu=True, name_prefix=\"a\")\n",
        "resource_pool_merge = merge_resource_pool(resource_pool, resource_pool_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05c2e305",
      "metadata": {
        "tags": [],
        "id": "05c2e305"
      },
      "outputs": [],
      "source": [
        "# Establish a RayWorkerGroup on the newly created resource pool.\n",
        "worker_group_1 = RayWorkerGroup(resource_pool_1, class_with_args)\n",
        "worker_group_merge = RayWorkerGroup(resource_pool_merge, class_with_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b9b13f4",
      "metadata": {
        "tags": [],
        "id": "6b9b13f4"
      },
      "outputs": [],
      "source": [
        "# Run 'add' on the second set of 4 GPUs; the result should be [2, 3, 4, 5].\n",
        "output_1 = worker_group_1.execute_all_sync(\"add\", x=[2, 2, 2, 2])\n",
        "print(output_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d856d030",
      "metadata": {
        "tags": [],
        "id": "d856d030"
      },
      "outputs": [],
      "source": [
        "# Run 'add' on the merged set of 8 GPUs; the result should be [3, 4, 5, 6, 7, 8, 9, 10].\n",
        "output_merge = worker_group_merge.execute_all_sync(\"add\", x=[3, 3, 3, 3, 3, 3, 3, 3])\n",
        "print(output_merge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33a4628c",
      "metadata": {
        "tags": [],
        "id": "33a4628c"
      },
      "outputs": [],
      "source": [
        "print(worker_group.world_size, worker_group_1.world_size, worker_group_merge.world_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df19d13",
      "metadata": {
        "id": "3df19d13"
      },
      "source": [
        "## Chapter 3: Data Dispatch, Execution and Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acb22d9d",
      "metadata": {
        "id": "acb22d9d"
      },
      "source": [
        "In the above example, we used the `execute_all_sync` function in the RayWorkerGroup to dispatch data from the driver to each worker. This is very inconvenient for coding.\n",
        "In this chapter, we use the form of function decorators to allow RayWorkerGroup to directly call functions written in the Worker, and to greatly simplify parameter passing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35237432",
      "metadata": {
        "tags": [],
        "id": "35237432"
      },
      "outputs": [],
      "source": [
        "from verl.single_controller.base.decorator import Dispatch, Execute, register"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88b8ba3b",
      "metadata": {
        "tags": [],
        "id": "88b8ba3b"
      },
      "outputs": [],
      "source": [
        "@ray.remote\n",
        "class GPUAccumulatorDecorator(Worker):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        # The initial value of each rank is the same as the rank\n",
        "        self.value = torch.zeros(size=(1,), device=\"cuda\") + self.rank\n",
        "\n",
        "    # map from a single input to all the worker\n",
        "    @register(Dispatch.ONE_TO_ALL)\n",
        "    def add(self, x):\n",
        "        print(x)\n",
        "        self.value = self.value + x\n",
        "        print(f\"rank {self.rank}, value: {self.value}\")\n",
        "        return self.value.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eddaa043",
      "metadata": {
        "tags": [],
        "id": "eddaa043"
      },
      "outputs": [],
      "source": [
        "class_with_args = RayClassWithInitArgs(cls=GPUAccumulatorDecorator)\n",
        "gpu_accumulator_decorator = RayWorkerGroup(resource_pool_merge, class_with_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10087c91",
      "metadata": {
        "tags": [],
        "id": "10087c91"
      },
      "outputs": [],
      "source": [
        "# As we can see, 10 is automatically dispatched to each Worker in this RayWorkerGroup.\n",
        "print(gpu_accumulator_decorator.add(x=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "540ee6ad",
      "metadata": {
        "id": "540ee6ad"
      },
      "source": [
        "### Custom Dispatch, Collection\n",
        "Users can customize `dispatch` and `collection` function. You only need to write the `dispatch_fn` and `collect_fn` functions yourself. We also support executing RPC only on rank_zero, with specific examples provided below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e041270",
      "metadata": {
        "tags": [],
        "id": "8e041270"
      },
      "outputs": [],
      "source": [
        "from verl.single_controller.base.decorator import Dispatch, collect_all_to_all, register"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43b5be31",
      "metadata": {
        "tags": [],
        "id": "43b5be31"
      },
      "outputs": [],
      "source": [
        "def two_to_all_dispatch_fn(worker_group, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Assume the input is a list of 2. Duplicate the input interleaved and pass to each worker.\n",
        "    \"\"\"\n",
        "    for arg in args:\n",
        "        assert len(arg) == 2\n",
        "        for i in range(worker_group.world_size - 2):\n",
        "            arg.append(arg[i % 2])\n",
        "    for k, v in kwargs.items():\n",
        "        assert len(v) == 2\n",
        "        for i in range(worker_group.world_size - 2):\n",
        "            v.append(v[i % 2])\n",
        "    return args, kwargs\n",
        "\n",
        "\n",
        "@ray.remote\n",
        "class TestActor(Worker):\n",
        "    # TODO: pass *args and **kwargs is bug prone and not very convincing\n",
        "    def __init__(self, x) -> None:\n",
        "        super().__init__()\n",
        "        self._x = x\n",
        "\n",
        "    def foo(self, y):\n",
        "        return self._x + y\n",
        "\n",
        "    @register(dispatch_mode=Dispatch.ALL_TO_ALL, execute_mode=Execute.RANK_ZERO)\n",
        "    def foo_rank_zero(self, x, y):\n",
        "        return self._x + y + x\n",
        "\n",
        "    @register(dispatch_mode={\"dispatch_fn\": two_to_all_dispatch_fn, \"collect_fn\": collect_all_to_all})\n",
        "    def foo_custom(self, x, y):\n",
        "        return self._x + y + x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83ec6609",
      "metadata": {
        "tags": [],
        "id": "83ec6609"
      },
      "outputs": [],
      "source": [
        "class_with_args = RayClassWithInitArgs(cls=TestActor, x=2)\n",
        "worker_group = RayWorkerGroup(resource_pool, class_with_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62c58d8a",
      "metadata": {
        "tags": [],
        "id": "62c58d8a"
      },
      "outputs": [],
      "source": [
        "output_ref = worker_group.foo_custom(x=[1, 2], y=[5, 6])\n",
        "assert output_ref == [8, 10, 8, 10]\n",
        "\n",
        "output_ref = worker_group.foo_rank_zero(x=1, y=2)\n",
        "assert output_ref == 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14689353",
      "metadata": {
        "tags": [],
        "id": "14689353"
      },
      "outputs": [],
      "source": [
        "print(gpu_accumulator_decorator.world_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c80bbf4",
      "metadata": {
        "tags": [],
        "id": "2c80bbf4"
      },
      "outputs": [],
      "source": [
        "# Shutdown ray cluster\n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5c8151c",
      "metadata": {
        "id": "a5c8151c"
      },
      "source": [
        "## Chapter 4: NVMegatronRayWorkerGroup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd5680e9",
      "metadata": {
        "id": "cd5680e9"
      },
      "source": [
        "Due to the Ray issue, we can only support max_colocate_count=1 in RayResourcePool for now.\n",
        "This means that each GPU can only have one process.\n",
        "We can support max_colocate > 1 when applying this pull request: https://github.com/ray-project/ray/pull/44385"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92724419",
      "metadata": {
        "id": "92724419"
      },
      "source": [
        "Therefore, we need to restart the ray and initialize a new resource_pool to demonstrate the **NVMegatronRayWorkerGroup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b038538",
      "metadata": {
        "tags": [],
        "id": "9b038538"
      },
      "outputs": [],
      "source": [
        "# Build a local ray cluster. The head node and worker node are on this machine\n",
        "ray.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebfd8798",
      "metadata": {
        "id": "ebfd8798"
      },
      "source": [
        "Finally, we implement a `NVMegatronRayWorkerGroup`, within which we create a Megatron and then run a tensor parallel (tp) split Llama mlp layer. Here, we use a complex dispatch mode, `Megatron_COMPUTE`. This dispatch mode assumes that user passes the data partitioned by DP dimension. The data is dispatched to all tp/pp ranks within the same dp group, and ultimately only collects output data from tp=0 and the last pp. In this way, for users that only write code on the driver, the Megatron behind the RPC becomes transparent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a032154",
      "metadata": {
        "tags": [],
        "id": "5a032154"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "current_pythonpath = os.environ.get(\"PYTHONPATH\", \"\")\n",
        "\n",
        "new_path = \"/opt/tiger/Megatron-LM\"\n",
        "\n",
        "new_pythonpath = f\"{new_path}:{current_pythonpath}\" if current_pythonpath else new_path\n",
        "\n",
        "os.environ[\"PYTHONPATH\"] = new_pythonpath\n",
        "\n",
        "print(new_path)\n",
        "sys.path.append(new_path)\n",
        "\n",
        "import megatron\n",
        "\n",
        "print(megatron.__file__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c84cd5a",
      "metadata": {
        "tags": [],
        "id": "8c84cd5a"
      },
      "outputs": [],
      "source": [
        "from megatron.core import parallel_state as mpu\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from verl.single_controller.base.decorator import Dispatch, Execute, register\n",
        "from verl.single_controller.base.megatron.worker import MegatronWorker\n",
        "from verl.single_controller.ray.base import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup\n",
        "from verl.single_controller.ray.megatron import NVMegatronRayWorkerGroup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1debcc",
      "metadata": {
        "tags": [],
        "id": "1b1debcc"
      },
      "outputs": [],
      "source": [
        "resource_pool = RayResourcePool([4], use_gpu=True, max_colocate_count=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bccbe081",
      "metadata": {
        "tags": [],
        "collapsed": true,
        "id": "bccbe081"
      },
      "outputs": [],
      "source": [
        "@ray.remote\n",
        "class MLPLayerWorker(MegatronWorker):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        rank = int(os.environ[\"LOCAL_RANK\"])\n",
        "        torch.distributed.init_process_group(backend=\"nccl\")\n",
        "        torch.cuda.set_device(rank)\n",
        "\n",
        "        mpu.initialize_model_parallel(\n",
        "            tensor_model_parallel_size=4,\n",
        "            pipeline_model_parallel_size=1,\n",
        "            virtual_pipeline_model_parallel_size=None,\n",
        "            pipeline_model_parallel_split_rank=None,\n",
        "            use_sharp=False,\n",
        "            context_parallel_size=1,\n",
        "            expert_model_parallel_size=1,\n",
        "            nccl_communicator_config_path=None,\n",
        "        )\n",
        "        from megatron.core import tensor_parallel\n",
        "\n",
        "        tensor_parallel.model_parallel_cuda_manual_seed(10)\n",
        "\n",
        "    @register(Dispatch.ONE_TO_ALL)\n",
        "    def init_model(self, config):\n",
        "        from omegaconf import OmegaConf\n",
        "\n",
        "        from verl.models.llama.megatron.layers import ParallelLlamaMLP\n",
        "        from verl.utils.megatron_utils import init_model_parallel_config\n",
        "\n",
        "        megatron_config = OmegaConf.create(\n",
        "            {\n",
        "                \"sequence_parallel\": False,\n",
        "                \"param_dtype\": \"fp32\",\n",
        "                \"tensor_model_parallel_size\": mpu.get_tensor_model_parallel_world_size(),\n",
        "                \"pipeline_model_parallel_rank\": mpu.get_pipeline_model_parallel_rank(),\n",
        "                \"pipeline_model_parallel_size\": mpu.get_pipeline_model_parallel_world_size(),\n",
        "                \"virtual_pipeline_model_parallel_rank\": mpu.get_virtual_pipeline_model_parallel_rank(),\n",
        "                \"virtual_pipeline_model_parallel_size\": mpu.get_virtual_pipeline_model_parallel_world_size(),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        megatron_config = init_model_parallel_config(megatron_config)\n",
        "        self.parallel_layer = ParallelLlamaMLP(config=config, megatron_config=megatron_config)\n",
        "\n",
        "    @register(Dispatch.ONE_TO_ALL)\n",
        "    def get_weights(self):\n",
        "        output = {}\n",
        "        for key, val in self.parallel_layer.named_parameters():\n",
        "            output[key] = val\n",
        "        return output\n",
        "\n",
        "    @register(Dispatch.MEGATRON_COMPUTE)\n",
        "    def run_layer(self, x):\n",
        "        x = x.to(\"cuda\")\n",
        "        y = self.parallel_layer(x)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a655271d",
      "metadata": {
        "tags": [],
        "id": "a655271d"
      },
      "outputs": [],
      "source": [
        "layer_cls = RayClassWithInitArgs(cls=MLPLayerWorker)\n",
        "layer_worker_group = NVMegatronRayWorkerGroup(\n",
        "    resource_pool=resource_pool,\n",
        "    ray_cls_with_init=layer_cls,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f105ebee",
      "metadata": {
        "tags": [],
        "id": "f105ebee"
      },
      "outputs": [],
      "source": [
        "print(layer_worker_group.world_size, layer_worker_group.tp_size, layer_worker_group.pp_size, layer_worker_group.dp_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38655091",
      "metadata": {
        "tags": [],
        "id": "38655091"
      },
      "outputs": [],
      "source": [
        "ffn_hidden_size = 11008\n",
        "batch_size = 16\n",
        "seq_len = 2048\n",
        "hidden_size = 4096\n",
        "\n",
        "config = OmegaConf.create(\n",
        "    {\n",
        "        \"hidden_size\": hidden_size,\n",
        "        \"intermediate_size\": ffn_hidden_size,\n",
        "        \"hidden_act\": \"silu\",\n",
        "        \"pretraining_tp\": 1,\n",
        "        \"tp\": layer_worker_group.tp_size,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a026efca",
      "metadata": {
        "tags": [],
        "id": "a026efca"
      },
      "outputs": [],
      "source": [
        "x = torch.rand(size=(seq_len, batch_size, hidden_size), dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5fcaf13",
      "metadata": {
        "tags": [],
        "id": "f5fcaf13"
      },
      "outputs": [],
      "source": [
        "layer_worker_group.init_model(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f5cc9b4",
      "metadata": {
        "tags": [],
        "id": "3f5cc9b4"
      },
      "outputs": [],
      "source": [
        "output = layer_worker_group.run_layer(\n",
        "    [x]\n",
        ")  # This must be a list of size 1, ensuring that the input equals the data parallel (dp).\n",
        "print(output[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49792210",
      "metadata": {
        "tags": [],
        "id": "49792210"
      },
      "outputs": [],
      "source": [
        "# Shutdown ray cluster\n",
        "ray.shutdown()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "generative_ai_disabled": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}