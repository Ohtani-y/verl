# megatron actor 設定、trainer/config/actor/actor.yaml から継承
defaults:
  - actor
  - _self_

_target_: verl.workers.config.McoreActorConfig

strategy: megatron

data_loader_seed: null

load_weight: True

optim:
  _target_: verl.workers.config.McoreOptimizerConfig
  optimizer: adam

  clip_grad: 1.0

  lr_warmup_init: 0.0

  lr_decay_steps: null

  lr_decay_style: constant

  min_lr: 0.0

  weight_decay_incr_style: constant

  lr_wsd_decay_style: exponential

  lr_wsd_decay_steps: null

  use_checkpoint_opt_param_scheduler: False

megatron:

  _target_: verl.workers.config.McoreEngineConfig

  param_offload: False

  grad_offload: False

  optimizer_offload: False

  tensor_model_parallel_size: 1

  expert_model_parallel_size: 1

  expert_tensor_parallel_size: null

  pipeline_model_parallel_size: 1

  virtual_pipeline_model_parallel_size: null

  context_parallel_size: 1

  sequence_parallel: True

  use_distributed_optimizer: True

  use_dist_checkpointing: False

  dist_checkpointing_path: null

  # oc.select: ref.megatron.seed のデフォルト値
  seed: 42

  override_ddp_config: {}

  # oc.select: ref.megatron.override_transformer_config のデフォルト値
  override_transformer_config:
    # 再計算粒度、選択肢: ["full", "selective"]
    recompute_granularity: null

    # 再計算モジュール、複数選択: ["core_attn", "moe_act", "layernorm", "mla_up_proj", "mlp", "moe"]
    recompute_modules: ["core_attn"]

    # 'uniform', 'block'
    # 'uniform' は transformer レイヤーの総数を分割し、各チャンクの入力活性化をチェックポイント
    # 'block' は指定された粒度でパイプラインステージごとに指定された数のレイヤーをチェックポイント
    recompute_method: null

    recompute_num_layers: null

  # oc.select: ref.megatron.use_mbridge のデフォルト値
  use_mbridge: False

profile:

  use_profile: False

  # list, you can specify the ranks to profile
  profile_ranks: null

  # start step in update_policy
  step_start: -1

  # end step
  step_end: -1

  # the path to save the profile result
  save_path: null
